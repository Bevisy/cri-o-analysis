# conmon-rs 服务启动流程源码分析
conmonr-rs 是一个用 Rust 编写的 pod 级别的 OCI 容器运行时监视器，旨在扩展并取代现有的 conmon 功能，不仅监视单个容器，还监视整个 pod。通过 UNIX 域套接字处理新容器和执行进程的请求，提升了管理效率。

本文主要分析 conmon-rs 服务的启动流程。

涉及源码版本：[v0.6.3](https://github.com/containers/conmon-rs/tree/v0.6.3)

### 1. 函数入口
```rust
// conmon-rs/server/src/main.rs:4
fn main() -> Result<()> {
    Server::new()
        .context("create server")?
        .start()
        .context("start server")
}
```
主函数入口，先执行 Server.new() 创建 server 实例，随后调用 Server.start() 启动 conmonrs 服务。

crio 创建容器启动 conmonrs进程，启动参数如下：
```
/usr/bin/crio-conmonrs --runtime /usr/bin/crio-crun --runtime-dir /var/lib/containers/storage/overlay-containers/c614c685fba3f0d314943cc9bc7f43eeda209c4598c519c59b960a9a7ededed4/userdata --runtime-root /run/crun --log-level info --log-driver systemd --cgroup-manager systemd
```

### 2. 分析 Server.new()
进入 new() 函数：
```
Breakpoint 2, conmonrs::server::Server::new () at conmon-rs/server/src/server.rs:61
61	        let server = Self {
```
初始化 server，值为：
```
{
    config: conmonrs::config::Config {command: core::option::Option<conmonrs::config::Commands>::None, version: core::option::Option<conmonrs::config::Verbosity>::None, log_level: conmonrs::config::LogLevel::Info, log_driver: conmonrs::config::LogDriver::Systemd, runtime: std::path::PathBuf {inner: "/usr/bin/crio-crun"}, runtime_dir: std::path::PathBuf {inner: "/var/lib/containers/storage/overlay-containers/c614c685fba3f0d314943cc9bc7f43eeda209c4598c519c59b960a9a7ededed4/userdata"}, runtime_root: core::option::Option<std::path::PathBuf>::Some(std::path::PathBuf {inner: "/run/crun"}), skip_fork: false, cgroup_manager: "systemd", enable_tracing: false, tracing_endpoint: "http://localhost:4317"}, 
    
    reaper: Arc(strong=1, weak=0) = {value = conmonrs::child_reaper::ChildReaper {grandchildren: Arc(strong=1, weak=0) = {value = std::sync::mutex::Mutex<multimap::MultiMap<alloc::string::String, conmonrs::child_reaper::ReapableChild, std::hash::random::RandomState>> {inner: std::sys::sync::mutex::futex::Mutex {futex: core::sync::atomic::AtomicU32 {v: core::cell::UnsafeCell<u32> {value: 0}}}, poison: std::sync::poison::Flag {failed: core::sync::atomic::AtomicBool {v: core::cell::UnsafeCell<u8> {value: 0}}}, data: core::cell::UnsafeCell<multimap::MultiMap<alloc::string::String, conmonrs::child_reaper::ReapableChild, std::hash::random::RandomState>> {value: multimap::MultiMap<alloc::string::String, conmonrs::child_reaper::ReapableChild, std::hash::random::RandomState> {inner: HashMap(size=0)}}}, strong = 1, weak = 0}}, strong = 1,weak = 0}, 
    
    fd_socket: Arc(strong=1, weak=0) = {value = conmonrs::fd_socket::FdSocket {server: tokio::sync::mutex::Mutex<core::option::Option<conmonrs::fd_socket::Server>> {s: tokio::sync::batch_semaphore::Semaphore {waiters: tokio::loom::std::mutex::Mutex<tokio::sync::batch_semaphore::Waitlist> (std::sync::mutex::Mutex<tokio::sync::batch_semaphore::Waitlist> {inner: std::sys::sync::mutex::futex::Mutex {futex: core::sync::atomic::AtomicU32 {v: core::cell::UnsafeCell<u32> {value: 0}}}, poison: std::sync::poison::Flag {failed: core::sync::atomic::AtomicBool {v: core::cell::UnsafeCell<u8> {value: 0}}}, data: core::cell::UnsafeCell<tokio::sync::batch_semaphore::Waitlist> {value: tokio::sync::batch_semaphore::Waitlist {queue: tokio::util::linked_list::LinkedList<tokio::sync::batch_semaphore::Waiter, tokio::sync::batch_semaphore::Waiter> {head: core::option::Option<core::ptr::non_null::NonNull<tokio::sync::batch_semaphore::Waiter>>::None, tail: core::option::Option<core::ptr::non_null::NonNull<tokio::sync::batch_semaphore::Waiter>>::None, _marker: core::marker::PhantomData<*const tokio::sync::batch_semaphore::Waiter>}, closed: false}}}), permits: tokio::loom::std::atomic_usize::AtomicUsize {inner: core::cell::UnsafeCell<core::sync::atomic::AtomicUsize> {value: core::sync::atomic::AtomicUsize {v: core::cell::UnsafeCell<usize> {value: 2}}}}}, c: core::cell::UnsafeCell<core::option::Option<conmonrs::fd_socket::Server>> {value: core::option::Option<conmonrs::fd_socket::Server>::None}}, state: std::sync::mutex::Mutex<conmonrs::fd_socket::State> {inner: std::sys::sync::mutex::futex::Mutex {futex: core::sync::atomic::AtomicU32 {v: core::cell::UnsafeCell<u32> {value: 0}}}, poison: std::sync::poison::Flag {failed: core::sync::atomic::AtomicBool {v: core::cell::UnsafeCell<u8> {value: 0}}}, data: core::cell::UnsafeCell<conmonrs::fd_socket::State> {value: conmonrs::fd_socket::State {last: core::num::wrapping::Wrapping<u64> (0), fds: HashMap(size=0)}}}}, strong = 1, weak = 0}}
}
```
server 值包含三部分 config、reaper、fd_socket，含义分别为：
config: 服务实例配置
reaper: reaper 实例，负责子进程管理
fd_socket: Fd socket，用于建立 capnp rpc 连接

### 3. 继续分析 Server.new()
```
Line 67 of "conmon-rs/server/src/server.rs"(结合断点信息和行号可推断代码位置，如果不标明文件路径，将默认与上一代码块同一函数或者文件路径)
67	        if let Some(v) = server.config().version() {
68	            Version::new(v == Verbosity::Full).print();
69	            process::exit(0);
70	        }
```
此处解析 version字段，如果为 full，则打印 conmonrs 版本信息到 stdout，随后退出进程（没看明白这样做的意义）。此处实际值为 None，继续下一步：

```
72	        if let Some(Commands::Pause {
73	            base_path,
74	            pod_id,
75	            ipc,
76	            pid,
77	            net,
78	            user,
79	            uts,
80	            uid_mappings,
81	            gid_mappings,
82	        }) = server.config().command()
83	        {
84	            Pause::run(
85	                base_path,
86	                pod_id,
87	                *ipc,
88	                *pid,
89	                *net,
90	                *user,
91	                *uts,
92	                uid_mappings,
93	                gid_mappings,
94	            )
95	            .context("run pause")?;
96	            process::exit(0);
97	        }
```
此处，执行 server.config().command() 获取枚举 Command 值为 Pasue 则运行 pause，根据上述 server 初始化值，此处不存在枚举值 Command::Pause，所以不会执行。继续执行。

```
99	        server.config().validate().context("validate config")?;
```
此处为 server 配置校验：
1. 校验 runtime、runtime--root 必需值
2. runtime-root 目录不存在则创建
3. 校验 conmon.socket、conmon-fs.socket 是否存在，存在则删除

继续执行：
```
101	        Self::init().context("init self")?;
```
服务配置初始化，包括：重置 locale、设置默认 mask 为 0o022、oom_score 为 -1000.

```
102	        Ok(server)
```
返回 server 实例，类型为枚举 Result::Ok。

### 4. Server 实例创建后，进入 Server.start() 阶段
```
Breakpoint 3, conmonrs::server::Server::start (self=...) at conmon-rs/server/src/server.rs:112
112	        if !self.config().skip_fork() {
```
skip_fork 值默认为 false，所以会执行 frok()：
```
113	            match unsafe { fork()? } {
[Detaching after fork from child process 31315]
```
创建新的进程 31315 如下：
```
  28633   28638   28638   25568 pts/3      27756 t        0   0:00          |                                   \_ /usr/bin/crio-conmonrs --runtime /usr/bin/crio-crun --runtime-dir /var/lib/containers/storage/overlay-containers/c614c685fba3f0d314943cc9bc7f43eeda209c4598c519c59b960a9a7ededed4/userdata --runtime-root /run/crun --log-level info --log-driver systemd --cgroup-manager systemd
  28638   31315   28638   25568 pts/3      27756 Sl       0   0:00          |                                       \_ /usr/bin/crio-conmonrs --runtime /usr/bin/crio-crun --runtime-dir /var/lib/containers/storage/overlay-containers/c614c685fba3f0d314943cc9bc7f43eeda209c4598c519c59b960a9a7ededed4/userdata --runtime-root /run/crun --log-level info --log-driver systemd --cgroup-manager systemd
```
此处为什么这样做的原因在于：如果不在设置 tokio 之前 fork，child（子任务）会处于一个奇怪的线程空间，可能会面临死锁。

fork() 后产生两个进程，分别为 Parent 和 Child，Parent 分支写入进程 pid 到 conmon pidfile，随后执行 \_exit(0) 父进程退出。Child 分支继续执行，最终，我们看到新建的子进程由1号接管，其父进程退出。
```rust
            match unsafe { fork()? } {
                ForkResult::Parent { child, .. } => {
                    let child_str = format!("{child}");
                    File::create(self.config().conmon_pidfile())?
                        .write_all(child_str.as_bytes())?;
                    unsafe { _exit(0) };
                }
                ForkResult::Child => (),
            }
```

查看节点进程，子进程由 1 号进程接管，其父进程退出：
```
      1   31315   28638   25568 pts/3      27756 Sl       0   0:00 /usr/bin/crio-conmonrs --runtime /usr/bin/crio-crun --runtime-dir /var/lib/containers/storage/overlay-containers/c614c685fba3f0d314943cc9bc7f43eeda209c4598c519c59b960a9a7ededed4/userdata --runtime-root /run/crun --log-level info --log-driver systemd --cgroup-manager systemd
```

此时，节点上 conmonrs 进程作为一个 daemon 服务进程存在。

此时，查看 conmonrs 的 runtime dir，会发现存在 conmon.sock 和 config.json 文件，但是不存在 pidfile（暂不清楚具体原因）：
```
# ls -lsh /var/lib/containers/storage/overlay-containers/c614c685fba3f0d314943cc9bc7f43eeda209c4598c519c59b960a9a7ededed4/userdata/
total 20K
20K -rw-r--r-- 1 root root 18K Jun  9 21:29 config.json
  0 srwxr-xr-x 1 root root   0 Jun  9 23:35 conmon.sock
```

进一步查看进程内的线程信息如下：
```
# ps -L -p 31315
    PID     LWP TTY          TIME CMD
  31315   31315 pts/3    00:00:00 crio-conmonrs
  31315   31316 pts/3    00:00:00 tokio-runtime-w
  31315   31317 pts/3    00:00:00 tokio-runtime-w
  31315   31318 pts/3    00:00:00 tokio-runtime-w
  31315   31319 pts/3    00:00:00 tokio-runtime-w
  31315   31320 pts/3    00:00:00 tokio-runtime-w
```

### 5. fork() 后，继续执行 Server.start()
执行 fork() 后，新 conmonrs 进程 31315 类型为 child，继续 Server.start() 逻辑。
```rust
        // now that we've forked, set self to childreaper
        prctl::set_child_subreaper(true)
            .map_err(errno::from_i32)
            .context("set child subreaper")?;
```
设置此进程为子进程收割者（意味着此进程的子进程成为孤儿进程后，由此进程接管，而不是由 1 号进程接管。子进程成为孤儿进程的条件是父进程异常退出，所以此处应该理解为此进程可接管”孙“进程。）继续：

```rust
        let enable_tracing = self.config().enable_tracing();
```
设置 enable_tracing 配置，默认未开启：
```
# ps -eo pid,ppid,cmd --width 1000
  31315       1 /usr/bin/crio-conmonrs --runtime /usr/bin/crio-crun --runtime-dir /var/lib/containers/storage/overlay-containers/c614c685fba3f0d314943cc9bc7f43eeda209c4598c519c59b960a9a7ededed4/userdata --runtime-root /run/crun --log-level info --log-driver systemd --cgroup-manager systemd
```

继续执行，此处构建一个配置为多线程并启用了所有功能的异步运行时：
```rust
        let rt = Builder::new_multi_thread().enable_all().build()?;
        rt.block_on(self.spawn_tasks())?;
```
Builder::new_multi_thread() 调用创建了一个运行时构建器（Builder），这个构建器配置为多线程模式。多线程运行时意味着它可以在多个线程上并发地执行多个任务。接下来的 enable_all() 调用是对构建器的配置，它启用了运行时的所有功能，包括定时器、IO操作等，确保运行时具备执行异步任务所需的所有能力。最后，build() 尝试构建配置好的运行时。这个方法可能会失败（例如，因为系统资源不足），所以它返回一个Result类型，这是 Rust 中用于表示可能失败的操作的类型。? 操作符是一种错误传播的快捷方式，如果 build() 方法返回错误，那么这个错误会被立即返回给调用者，否则成功构建的运行时会被返回。

此处，构建器 rt 创建成功，随后拉起 self.spawn_tasks()。

#### 5.1 分析 Server.spawn_tasks()
```rust
// conmon-rs/server/src/server.rs:196
    /// Spawns all required tokio tasks.
    async fn spawn_tasks(self) -> Result<()> {
        self.init_logging().context("init logging")?;

        let (shutdown_tx, shutdown_rx) = oneshot::channel();
        let socket = self.config().socket();
        let fd_socket = self.config().fd_socket();
        let reaper = self.reaper.clone();
```
新建单次发送 channel 用于在信号处理器和后台任务之间传递关闭信号，shutdown_tx 是发送端，shutdown_rx 是接收端），并从 server 实例获取参数 socket、fd_socket，克隆出新的对象 reaper 等。

```rust
        let signal_handler_span = debug_span!("signal_handler");
        task::spawn(
            Self::start_signal_handler(reaper, socket, fd_socket, shutdown_tx)
                .with_context(signal_handler_span.context())
                .instrument(signal_handler_span),
        );
```
通过 task::spawn 异步启动信号处理任务。此任务通过调用 Self::start_signal_handler 函数并传入之前获取的 reaper、socket、fd_socket 和 shutdown_tx。关于函数 start_signal_handler 的具体分析，先放在下一节，此处继续分析 spawn_tasks()。

```rust
        let backend_span = debug_span!("backend");
        task::spawn_blocking(move || {
            Handle::current().block_on(
                LocalSet::new()
                    .run_until(self.start_backend(shutdown_rx))
                    .with_context(backend_span.context())
                    .instrument(backend_span),
            )
        })
        .await?
```
使用 task::spawn_blocking 启动了一个阻塞的后台任务。这是通过创建一个新的 LocalSet 实例，并在其上调用run_until 方法来实现的，run_until 方法将等待 self.start_backend(shutdown_rx) 的完成。此处，move 的作用是捕获闭包内部使用的外部变量的所有权，此处为 shutdown_rx、backend_span。最后，通过 .await? 等待后台任务完成，并处理可能发生的错误。

#### 5.2 分析 Server.start_backend()
继续分析  start_backend()：
```rust
// conmon-rs/server/src/server.rs:276
    async fn start_backend(self, mut shutdown_rx: oneshot::Receiver<()>) -> Result<()> {
        let listener =
            Listener::<DefaultListener>::default().bind_long_path(self.config().socket())?;
        let client: conmon::Client = capnp_rpc::new_client(self);
```

新建 listener，基于文件 conmon.sock 创建的网络监听器。新建 client，通过调用 capnp_rpc::new_client(self) 创建 capnp_rpc 的客户端，由于 server 实例本身已经实现 conmon::Server trait，所以此 client 包含全部 capnp 定义的 rpc 函数，如下：
```rust
  pub trait Server<>   {
    fn version(&mut self, _: VersionParams<>, _: VersionResults<>) -> ::capnp::capability::Promise<(), ::capnp::Error> { ::capnp::capability::Promise::err(::capnp::Error::unimplemented("method conmon::Server::version not implemented".to_string())) }
    fn create_container(&mut self, _: CreateContainerParams<>, _: CreateContainerResults<>) -> ::capnp::capability::Promise<(), ::capnp::Error> { ::capnp::capability::Promise::err(::capnp::Error::unimplemented("method conmon::Server::create_container not implemented".to_string())) }
    fn exec_sync_container(&mut self, _: ExecSyncContainerParams<>, _: ExecSyncContainerResults<>) -> ::capnp::capability::Promise<(), ::capnp::Error> { ::capnp::capability::Promise::err(::capnp::Error::unimplemented("method conmon::Server::exec_sync_container not implemented".to_string())) }
    fn attach_container(&mut self, _: AttachContainerParams<>, _: AttachContainerResults<>) -> ::capnp::capability::Promise<(), ::capnp::Error> { ::capnp::capability::Promise::err(::capnp::Error::unimplemented("method conmon::Server::attach_container not implemented".to_string())) }
    fn reopen_log_container(&mut self, _: ReopenLogContainerParams<>, _: ReopenLogContainerResults<>) -> ::capnp::capability::Promise<(), ::capnp::Error> { ::capnp::capability::Promise::err(::capnp::Error::unimplemented("method conmon::Server::reopen_log_container not implemented".to_string())) }
    fn set_window_size_container(&mut self, _: SetWindowSizeContainerParams<>, _: SetWindowSizeContainerResults<>) -> ::capnp::capability::Promise<(), ::capnp::Error> { ::capnp::capability::Promise::err(::capnp::Error::unimplemented("method conmon::Server::set_window_size_container not implemented".to_string())) }
    fn create_namespaces(&mut self, _: CreateNamespacesParams<>, _: CreateNamespacesResults<>) -> ::capnp::capability::Promise<(), ::capnp::Error> { ::capnp::capability::Promise::err(::capnp::Error::unimplemented("method conmon::Server::create_namespaces not implemented".to_string())) }
    fn start_fd_socket(&mut self, _: StartFdSocketParams<>, _: StartFdSocketResults<>) -> ::capnp::capability::Promise<(), ::capnp::Error> { ::capnp::capability::Promise::err(::capnp::Error::unimplemented("method conmon::Server::start_fd_socket not implemented".to_string())) }
  }
```
server 实例实现 conmon::Server trait 位置 conmon-rs/server/src/rpc.rs:71。
```
impl conmon::Server for Server {
    /// Retrieve version information from the server.
    fn version(
        &mut self,
        params: conmon::VersionParams,
        mut results: conmon::VersionResults,
...
```

继续：
```rust
        loop {
            let stream = tokio::select! {
                _ = &mut shutdown_rx => {
                    debug!("Received shutdown message");
                    return Ok(())
                }
                stream = listener.accept() => {
                    stream?.0
                },
            };
            let (reader, writer) = TokioAsyncReadCompatExt::compat(stream).split();
            let network = Box::new(VatNetwork::new(
                reader,
                writer,
                Side::Server,
                Default::default(),
            ));
            let rpc_system = RpcSystem::new(network, Some(client.clone().client));
            task::spawn_local(Box::pin(rpc_system.map(|_| ())));
        }
```
此处进入无限循环体内，tokio::select! 宏等待两类事件中的任何一种：一是接收到关闭信号（通过shutdown_rx通道），二是成功接受一个新的网络连接（通过listener.accept()）。如果接收到关闭信号，它会记录一条调试信息并优雅地退出循环。如果接受到一个新的连接，它会解构这个连接以获取底层的流（stream）。
接下来，这个流被分割成读写两部分，这是通过 TokioAsyncReadCompatExt::compat(stream).split() 实现的，这允许流同时支持异步读和写操作。然后，使用这两部分创建了一个 VatNetwork 实例，这是一个网络层的抽象，它被配置为服务器端并使用默认设置。
之后，使用这个网络层实例创建了一个 RpcSystem，这是一个处理 RPC 调用的系统。它被配置为使用之前创建的 VatNetwork 实例和一个客户端实例（通过 client.clone().client 获取）。最后，这个 RPC 系统被封装在一个异步任务中，并通过 task::spawn_local 函数启动一个新的线程，至此，conmonrs 进程就可以并行地处理 RPC 调用。

#### 5.3 分析 Server.start_signal_handler()
```rust
// conmon-rs/server/src/server.rs:223
        let mut sigterm = signal(SignalKind::terminate())?;
        let mut sigint = signal(SignalKind::interrupt())?;
        let handled_sig: Signal;

        tokio::select! {
            _ = sigterm.recv() => {
                info!("Received SIGTERM");
                handled_sig = Signal::SIGTERM;
            }
            _ = sigint.recv() => {
                info!("Received SIGINT");
                handled_sig = Signal::SIGINT;
            }
        }
```
此处创建了两个信号接收器 sigterm 和 sigint，分别负责接收信号 SIGTERM 和 SIGINT。通过 tokio::select!宏等待这些信号之一的到来。一旦接收到信号，它会记录接收到的信号类型，并执行一系列清理操作。

```rust
        debug!("Starting grandchildren cleanup task");
        reaper
            .kill_grandchildren(handled_sig)
            .context("unable to kill grandchildren")?;

        debug!("Sending shutdown message");
        shutdown_tx
            .send(())
            .map_err(|_| format_err!("unable to send shutdown message"))?;

        debug!("Removing socket file {}", socket.as_ref().display());
        fs::remove_file(socket)
            .await
            .context("remove existing socket file")?;

        debug!("Removing fd socket file {}", fd_socket.as_ref().display());
        fs::remove_file(fd_socket)
            .await
            .or_else(|err| {
                if err.kind() == std::io::ErrorKind::NotFound {
                    Ok(())
                } else {
                    Err(err)
                }
            })
            .context("remove existing fd socket file")
```
清理操作包括调用 Pause::maybe_shared 方法尝试获取一个全局的Pause实例，并调用其stop方法来卸载命名空间并杀死相关的进程。接着，调用 reaper 的kill_grandchildren 方法来杀死所有子进程的子进程（即孙进程）。然后，通过 shutdown_tx 发送一个关闭信号，以通知其他部分程序（start_backend 启动的服务）正在关闭。最后，函数尝试删除 socket 和 fd_socket 指定的文件。